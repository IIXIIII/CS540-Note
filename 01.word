GPT stands for Generative Pre-trained Transformer.

âž© Unsupervised learning (convert text to numerical vectors).
    not giving answer/lable to the module, (just some questions)
      first week (W1)

âž© Supervised learning: (1) discriminative (predict answers based on questions), (2) generative (predict next word based on previous word).
    question answer pairs. find the relation between answer and questions. 
      (W2 - midterm, halfe of the course)
    discriminative - directly match questions with answers. x->y
    generative - given the answer, estimate what kind of question it is asking. (generate questions).  y->x
    There is a right answer 

âž© Reinforcement learning (update model based on human feedback).
    no course, but most of the stuff
    used to updata the modole based on the human feedback 
    give some kind of revoid by choosing the best action 
    No Correct answer, just the best answer


# N-gram Model : (small language module) the sentence is generated by looking at the previous sentence, base on the probability of the words. 
  The first Module covered in the lec. 
ðŸ“— A sentence is a sequence of words (tokens). Each unique word token is called a word type. The set of word types of called the vocabulary.

ðŸ“— A sentence with length d can be represented by (w1, w2, w3, w4,,,,,,wd)

ðŸ“— The probability of observing a word wt at position t of the sentence can be written as P(wt) (or in statistics P(Wt = wt)).

bag of words features: I an Grooot We are Love you guys
x1 = ()
x2=
x3=  

TF-IDF freatures: log (totoal # of documents/ #documents containg i )

x1 = (log (3/3))





