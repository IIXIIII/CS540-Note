# **L22**
### **Reinforcement Learning**
used in situations where people cannot explain or just dont know the answer for.  
EX: walking, cant explain how.  
give them reward for doing certain things.  
Credit assignment is the hard part of the Reinforcement Learning, since it is hard to tell which action did get near the final goal.  
Balance of **exploration** and **exploitation**.    
ðŸ“— Unlike search problems, the agent needs to learn the reward or cost function.  
  

### **Multi Armed Bandit**  
find the balance of exploration and exploitation.  
EX:  
There are 3 restaurants, there are diff distribution of reward. each one got there own mean, standard deviation.  
300 days, goal is to max the reward that you get from eating those restaurants.  
1. **explore only** , 100 days for restaurant 1, 100 days for 2 and 100 days for 3.  
the result will just be the mean. 
2. **exploit only**, 1 for restaurant 1, 1 for 2 and 1 for 3, go the most rewarding one for the rest 297 times.  
The issue with this strat is that the rewarding is random, might get a low reward from a good restaurant.  
3. **Epsilon Greedy**, a balance of explore and exploit. at a certain time of the exploration we have he info to tell which one is the best
and it is time to do exploitation.  
Epsilon = 10%  
Which is, on a given day, we got 10% of visiting a restaurant that historically give the most positive reward,
and we have 10% of randomly visiting a place.  
4. **Epsilon First**, fist  T * Epsilon  day - exploring, the rest - exploiting.  
  
There is a unit of **regret**, which is the difference of the actual value and the **optimal**, yes, there is a optimal answer, if we view this question from god perspective.   
  
**Zero Regret** is if you live in that place forever, where the T become infinity, the regret/T is approaching 0.  
  
**More context about the problem,**:  
restaurants rewards: R1, R2, ... Rn  
3 times the number is the actual reward of the restaurant,  
standard deviation d,  
now if the first restaurant is 2.9 in mean of reward and the second one is 3.0,
but the first one we only visited 1 time and the second of we visited for 100 times, 
since the second one always give us a positive reward at those first couple of visiting time, for some reason  
then this might be a little off since the 2.9 for the first restaurant might be an outlier.  

5. **UCB** or the **Upper Confidence Bound**
at a give time t, which is the current time, pick restaurant r such that the following value get maximized.   
   (the mean reward that you got from r) + sqrt((2*ln(time that you are on))/ (Nt(r) or the time that we have go) )
  
basically it said that if the over all mean is alike or similar, then the less time we visit a restaurant the more likely that we will visit it.  

  
doing this is because there is a change that the mean that we have is problematic.  
  
# **L23**  
### **Markov Chain**  
EX:  
every day is either  
the weather only depend on the previous day,  
the transition is fully governed by the 4 probabilities, 
```
         sun  not-sun    Wt(weather on today)
sun      0.3  0.7
not-sun  0.5  0.5
Wt-1(weather on the previous day)
```
1. State Space
2. Markov Assumption, is that given all the previous date's weather, the day's weather only depends on that one previous day
3. transition matrix.   
  
**Key Idea**  
There will be a value for the state of the next day that stay the same, also known as converge.  
Day 1: 100% sunny, 0% cloudy  
Day infinite: pi1 sunny, pi2 cloudy,  

pi1 * P(still sunny) + pi2 * P(cloudy become sunny) = pi1, which is the P of still sunny  
pi1 + pi2 = 1  
then we find pi1 and pi2  
  
can be used in Language Processing, for example, in language, 
we have **Pronoun**ï¼Œ **verb**ï¼Œ **noun** and exct,  
Those things are just like weather states.  at the beginning model at least.  
Another application is the position of you, since your next position only depend on your current position.  
so it can be used to predict your next position.   
  

### **Markov Decision Process**  




















                    