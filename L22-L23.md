# **L22**
### **Reinforcement Learning**
used in situations where people cannot explain or just dont know the answer for.  
EX: walking, cant explain how.  
give them reward for doing certain things.  
Credit assignment is the hard part of the Reinforcement Learning, since it is hard to tell which action did get near the final goal.  
Balance of **exploration** and **exploitation**.    
ðŸ“— Unlike search problems, the agent needs to learn the reward or cost function.  
  

also there are two type of reinforcement learning: 
1. value based 
   2. this required a value function. which is the total reward.  
   3. then find the optimal police
2. police based

### **Multi Armed Bandit**  
find the balance of exploration and exploitation.  
EX:  
There are 3 restaurants, there are diff distribution of reward. each one got there own mean, standard deviation.  
300 days, goal is to max the reward that you get from eating those restaurants.  
1. **explore only** , 100 days for restaurant 1, 100 days for 2 and 100 days for 3.  
the result will just be the mean. 
2. **exploit only**, 1 for restaurant 1, 1 for 2 and 1 for 3, go the most rewarding one for the rest 297 times.  
The issue with this strat is that the rewarding is random, might get a low reward from a good restaurant.  
3. **Epsilon Greedy**, a balance of explore and exploit. at a certain time of the exploration we have he info to tell which one is the best
and it is time to do exploitation.  
Epsilon = 10%  
Which is, on a given day, we got 10% of visiting a restaurant that historically give the most positive reward,
and we have 10% of randomly visiting a place.  
4. **Epsilon First**, fist  T * Epsilon  day - exploring, the rest - exploiting.  
  
There is a unit of **regret**, which is the difference of the actual value and the **optimal**, yes, there is a optimal answer, if we view this question from god perspective.   
  
**Zero Regret** is if you live in that place forever, where the T become infinity, the regret/T is approaching 0.  
  
**More context about the problem,**:  
restaurants rewards: R1, R2, ... Rn  
3 times the number is the actual reward of the restaurant,  
standard deviation d,  
now if the first restaurant is 2.9 in mean of reward and the second one is 3.0,
but the first one we only visited 1 time and the second of we visited for 100 times, 
since the second one always give us a positive reward at those first couple of visiting time, for some reason  
then this might be a little off since the 2.9 for the first restaurant might be an outlier.  

5. **UCB** or the **Upper Confidence Bound**
at a give time t, which is the current time, pick restaurant r such that the following value get maximized.   
   (the mean reward that you got from r) + sqrt((2*ln(time that you are on))/ (Nt(r) or the time that we have go) )
  
basically it said that if the over all mean is alike or similar, then the less time we visit a restaurant the more likely that we will visit it.  

  
doing this is because there is a change that the mean that we have is problematic.  
  
# **L23**  
### **Markov Chain**  
EX:  
every day is either  
the weather only depend on the previous day,  
the transition is fully governed by the 4 probabilities, 
```
         sun  not-sun    Wt(weather on today)
sun      0.3  0.7
not-sun  0.5  0.5
Wt-1(weather on the previous day)
```
1. State Space
2. Markov Assumption, is that given all the previous date's weather, the day's weather only depends on that one previous day
3. transition matrix.   
  
**Key Idea**  
There will be a value for the state of the next day that stay the same, also known as converge.  
Day 1: 100% sunny, 0% cloudy  
Day infinite: pi1 sunny, pi2 cloudy,  

pi1 * P(still sunny) + pi2 * P(cloudy become sunny) = pi1, which is the P of still sunny  
pi1 + pi2 = 1  
then we find pi1 and pi2  
  
can be used in Language Processing, for example, in language, 
we have **Pronoun**ï¼Œ **verb**ï¼Œ **noun** and exct,  
Those things are just like weather states.  at the beginning model at least.  
Another application is the position of you, since your next position only depend on your current position.  
so it can be used to predict your next position.   
  

### **Markov Decision Process**  
the same as above  
  
### **Value Function**  
There are several thing that we want to know:  
1. the certain probability of an agent to take certain action from a given state -> Policies  
2. how good that is for the agent -> Value Function  
  
------  
1. state value function  V(s)
   2. state s -> V -> number (How good is it to be in state s)
2. state-action value function  Q(s,a) 
   3. state & action -> Q -> number (q value)
   , which means how good is it to be in state s and take action q  

```
Q  a1       a2       a3        a4     actions
s1 Q(s1,a1)
s2          Q(s2,a2)
s2
s3
s4

states
```
  
### **Q Learning**  
### **Q Function**  

Youtube video: https://www.youtube.com/watch?v=TiAXhVAZQl8&t=325s&ab_channel=CodeEmporium  
  
the is a value based reinforcement learning method.  
 the goal is to lean the Q value on the above table make the total reward max.  
  
EX: move to the right
**Observed Q value**: Bellman Equation: Q(S1,right)observed = R(S2) + gama * max future Q value for the state  
gama: discount factor: balance how we want to balance the future reward and the current reward.  
  
**temporal difference err**: the difference between the observed Q value and the Q value in the function.  
  
**Update Rule**: Q(s1,right) = Q(s1,right) + alpha * TD err  
alpha : the learning rate. higher the value faster learning sice bigger the update.  

**Off-police**


### **SARSA**  
An alternative to Q learning is SARSA (State Action Reward State Action). It uses a pre-specified action for the next period instead of the optimal action based on the current Q estimate  
âž© SARSA is an on-policy learning algorithm since it computes the Q function based on a fixed policy.





















                    