# Trigram module #
  calculate the probability that one word followed by another
  ex:
    I am Groot * 13 + We are groot
  Pr{I | Groot }  = probibility of I come after Groot / the times Groot ever appear
                  = 12 / 14
Laplace Smoothing:
  if some case never appper, then the denominator will be 0 ()
  ex:
    (12+1) / (14 + 5) (add 5 cause there are totoal of 5 words in the sentence.)

    we are adding 1 to each element. (in the example it is a 5x5 matrix so it should add 5 in the deno)

The estimated (trained ) transition matrixï¼š
   5 x 5 matrix 
      I am Groot We are 
I 
am
Groot
we
are 

the Laplace smoothing is always applyed.  (for some reason and I dont know why)

In A1, the first element should be space.  


==== next topic ====

# Supervised learning  (super, unsuper, reinforce)
  relationship fo answer and question pairs 

ðŸ“— A machine learning data set usually contains features (text, images, ... converted to numerical vectors) and labels (categories, converted to integers).

Feature: X (text or imagees converted ot numerical vectors )
Labels: Y (categoreis converted to Integers.)

Supervised learning: giving set (X,Y), estimate the prediction function Y = f(X)

Discriminative model estimates P{y|x}:
Generative model: P{x|y} and predicts P{y|x}


# Natural Language Processing
ðŸ“— When processing language data, documents need to be first turned into sequences of word tokens.
âž© Split the string by space and punctuation.
âž© Remove stop-words such as "the", "of", "a", "with".
âž© Lower case all characters.
âž© Stemming or lemmatization words: change "looks", "looked", "looking" to "look". (kind of like puting similar words into groups)
ðŸ“— Each document needs to be converted into a numerical vector for supervised learning tasks.
  and here are two ways of convering them:
âž© Bag of words: feature uses the number of occurrences of each word type: Wikipedia.
âž© TF-IDF: Term-Frequency Inverse-Document-Frequency (TF-IDF) feature adjusts for whether each word type appears in multiple documents: Wikipedia.




# Bag of words Feature (method of convertinig txt into deatures(matrix))
  just like the Unigram probability 
  tiems of the target word appear / the total words 

  ex:
    the previous example:
    vaector of 5 number, the P{I}, P{am}, P{Groot} ......

# TF IDF Features
  TF - term frequency
  IDF - Inverse Document Frequency

  TF: the same way as in the bag of words features. TF = cij/ci1+...+cim  
  IDF: IDFj = log(n/|{j:cij>0}|)

# Supervised Learning Tasks #
ðŸ“— If the documents are labeled, then a supervised learning task is: given a training set of document features (for example, bag of words, TF-IDF) and their labels, estimate a function that predicts the label for new documents.
âž© Given emails, predict whether they are spams or hams.
âž© Given comments, predict whether they are offensive or not.
âž© Given reviews, predict whether they are positive or negative.
âž© Given essays, predict the grade A, B, ... or F.
âž© Given documents, predict which language it is from.







