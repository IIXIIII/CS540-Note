GPT stands for Generative Pre-trained Transformer.

‚û© Unsupervised learning (convert text to numerical vectors).
    not giving answer/lable to the module, (just some questions)
      first week (W1)

‚û© Supervised learning: (1) discriminative (predict answers based on questions), (2) generative (predict next word based on previous word).
    question answer pairs. find the relation between answer and questions. 
      (W2 - midterm, halfe of the course)
    discriminative - directly match questions with answers. x->y
    generative - given the answer, estimate what kind of question it is asking. (generate questions).  y->x
    There is a right answer 

‚û© Reinforcement learning (update model based on human feedback).
    no course, but most of the stuff
    used to updata the modole based on the human feedback 
    give some kind of revoid by choosing the best action 
    No Correct answer, just the best answer


# N-gram Model : (small language module) the sentence is generated by looking at the previous sentence, base on the probability of the words. 
  The first Module covered in the lec. 
üìó A sentence is a sequence of words (tokens). Each unique word token is called a word type. The set of word types of called the vocabulary.

üìó A sentence with length d can be represented by (w1, w2, w3, w4,,,,,,wd)

üìó The probability of observing a word wt at position t of the sentence can be written as P(wt) (or in statistics P(Wt = wt)).


# Unigram Model

# Maximum Likelihood Estimation
    The cat sat on the mat 
	1.	Tokenization: [‚Äúthe‚Äù, ‚Äúcat‚Äù, ‚Äúsat‚Äù, ‚Äúon‚Äù, ‚Äúthe‚Äù, ‚Äúmat‚Äù]
	2.	Word Frequencies:
		the: 2
		cat: 1
		sat: 1
		on: 1
		mat: 1
	3.	Total Words: 6
	4.	Word Probabilities:
		P(the) = 2/6 = 0.333
		P(cat) = 1/6 = 0.167
		P(sat) = 1/6 = 0.167
		P(on) = 1/6 = 0.167
		P(mat) = 1/6 = 0.167

the order Does not matter

Ex: 
Pr{I} = c_I /#words = 13/ 3*14
Pr{am} = c_am / #words = 13/ 3*14
Pr{Groot} = 1/13
Pr{I am Groot} = Pr{I} * Pr{am} * Pr{Groot} 

not a good module bcs the order does not matter 


# Bigram Model
    order Matters
    Bigram module assumes Markov property 
    
    Calculatoin: (the probbability of the word sequence appear)/ (the word sequence appear)

    in the matrix, every matrix is a probability distribution. roll add up = 1 

    probability matrix, when there is a word, go to colomn of the word and then choose a word. and then ....


# Transition Matrix
    Linear Algebra

# Trigram Modele
    Laplace Smoothing

    Pr{am | I} = c_(I am) / c_I = 13/13 =1
    Pr{Groot | am} = c_(am Groot) / c_am = 13/13
    Pr{I | Groot} = 0

    Pr(I am Groot) = Pr{I} * Pr{am | I} * Pr {Goot | am } = 13/42 *1*1=13/42











bag of words features: I an Grooot We are Love you guys
x1 = ()
x2=
x3=  

TF-IDF freatures: log (totoal # of documents/ #documents containg i )

x1 = (log (3/3))





